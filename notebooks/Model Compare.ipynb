{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344badd1-4831-499b-b003-4285c95ddbf1",
   "metadata": {},
   "source": [
    "## Импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7022311-e194-4782-ab04-7e375936f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from functools import lru_cache\n",
    "from typing import Dict, Optional\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge \n",
    "from bert_score import BERTScorer\n",
    "import warnings\n",
    "import logging\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import logging\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54c79f-6dfb-4622-8007-e88310920076",
   "metadata": {},
   "source": [
    "## Tuned LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0282ff-ba49-410d-b4bd-da8dc5a36053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSettings:\n",
    "    \"\"\"Настройки для модели unsloth/Llama-3.2-1B-Instruct\"\"\"\n",
    "    def __init__(self):\n",
    "        # Путь к предобученной модели\n",
    "        self.model_path = \"../models/Llama-merged\"\n",
    "        \n",
    "        # Параметры квантизации\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # Шаблон для форматирования диалога\n",
    "        self.chat_template = \"\"\"{% for message in messages %}\n",
    "            <|im_start|>{{ message['role'] }}\\n{{ message['content'] }}<|im_end|>\n",
    "        {% endfor %}\n",
    "        <|im_start|>assistant\\n\"\"\"\n",
    "        \n",
    "        # Стандартные параметры генерации текста\n",
    "        self.default_generation_params = {\n",
    "            'max_new_tokens': 50,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'repetition_penalty': 1.1,\n",
    "            'do_sample': True\n",
    "        }\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def load_model_and_tokenizer(settings: ModelSettings):\n",
    "    # Инициализация токенизатора\n",
    "    tokenizer1 = AutoTokenizer.from_pretrained(settings.model_path)\n",
    "    \n",
    "    # Загрузка модели с квантизацией\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=settings.model_path,\n",
    "        quantization_config=settings.quantization_config,\n",
    "        device_map=\"auto\",  # Автоматическое распределение по устройствам\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Настройка шаблона чата\n",
    "    tokenizer1.chat_template = settings.chat_template\n",
    "    \n",
    "    return model1, tokenizer1\n",
    "\n",
    "\n",
    "def format_chat_prompt(prompt: str, tokenizer1) -> str:\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    return tokenizer1.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_assistant_response(full_response: str) -> str:\n",
    "    match = re.search(\n",
    "        pattern=r\"<\\|im_start\\|>assistant\\s*(.*?)(<\\|im_end\\|>|$)\",\n",
    "        string=full_response,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    return match.group(1).strip() if match else full_response\n",
    "\n",
    "\n",
    "def generate_tuned_llama_response(\n",
    "    user_prompt: str,\n",
    "    generation_params: Optional[Dict] = None,\n",
    "    settings: Optional[ModelSettings] = None\n",
    ") -> str:\n",
    "    try:\n",
    "        current_settings = settings or ModelSettings()\n",
    "        generation_params = {**current_settings.default_generation_params, **(generation_params or {})}\n",
    "        \n",
    "        model, tokenizer = load_model_and_tokenizer(current_settings)\n",
    "        \n",
    "        formatted_prompt = format_chat_prompt(user_prompt, tokenizer)\n",
    "        \n",
    "        # Исправлено здесь: model1 -> model, tokenizer1 -> tokenizer\n",
    "        text_generator = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,        # ← Исправлено\n",
    "            tokenizer=tokenizer, # ← Исправлено\n",
    "        )\n",
    "        \n",
    "        generation_result = text_generator(formatted_prompt, **generation_params)\n",
    "        full_response = generation_result[0]['generated_text']\n",
    "        \n",
    "        return extract_assistant_response(full_response)\n",
    "    \n",
    "    except Exception as error:\n",
    "        return f\"Ошибка при генерации ответа: {str(error)}\"\n",
    "\n",
    "\n",
    "# Стандартное использование\n",
    "response = generate_tuned_llama_response(\"you really think hes got hepe\")\n",
    "print(\"Стандартный ответ:\", response)\n",
    "\n",
    "# Кастомизированный пример\n",
    "custom_settings = ModelSettings()\n",
    "custom_settings.default_generation_params['temperature'] = 0.9\n",
    "\n",
    "custom_response = generate_tuned_llama_response(\n",
    "    user_prompt=\"Explain quantum computing in simple terms\",\n",
    "    generation_params={'max_new_tokens': 100},\n",
    "    settings=custom_settings\n",
    ")\n",
    "print(\"\\nКастомизированный ответ:\", custom_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccf55e-3160-4bc0-ac1d-2559adb347a3",
   "metadata": {},
   "source": [
    "## Vanilla LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a352915-63ca-4499-a024-126547783557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфигурация\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "CHAT_TEMPLATE = \"\"\"{% for message in messages %}\n",
    "<|im_start|>{{ message['role'] }}\n",
    "{{ message['content'] }}<|im_end|>\n",
    "{% endfor %}\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Глобальные переменные для модели и токенизатора\n",
    "model2, tokenizer2 = None, None\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Загрузка модели и токенизатора\"\"\"\n",
    "    global model2, tokenizer2  # Явно объявляем использование глобальных переменных\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        ),\n",
    "        attn_implementation=\"eager\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Загрузка токенизатора\n",
    "    tokenizer2 = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Настройка шаблона чата\n",
    "    tokenizer2.chat_template = CHAT_TEMPLATE\n",
    "    tokenizer2.pad_token = tokenizer2.eos_token\n",
    "\n",
    "def generate_vanilla_llama_response(prompt: str) -> str:\n",
    "    \"\"\"Генерация ответа с автоматическим извлечением\"\"\"\n",
    "    global model2, tokenizer2\n",
    "    \n",
    "    try:\n",
    "        # Форматирование промпта\n",
    "        formatted_prompt = tokenizer2.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Генерация\n",
    "        inputs = tokenizer2.encode(\n",
    "            formatted_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            add_special_tokens=False\n",
    "        ).to(model2.device)\n",
    "        \n",
    "        outputs = model2.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=25,\n",
    "            do_sample=True,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        # Декодирование с пропуском специальных токенов\n",
    "        full_text = tokenizer2.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Очистка от возможных остаточных тегов\n",
    "        cleaned_response = re.sub(\n",
    "            r\"<\\|im_start\\|>|<\\|im_end\\|>|assistant\\s*\", \n",
    "            \"\", \n",
    "            full_text, \n",
    "            flags=re.IGNORECASE\n",
    "        ).strip()\n",
    "        \n",
    "        return cleaned_response if cleaned_response else \"Пустой ответ\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Ошибка: {str(e)}\"\n",
    "\n",
    "# Инициализация модели при загрузке скрипта\n",
    "load_model_and_tokenizer()\n",
    "\n",
    "# Пример использования\n",
    "q2 = \"you really think hes got hepe\"\n",
    "print(f\"{q2}:\\n{generate_vanilla_llama_response(q2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d012bb-f128-4c7a-b784-e174db29a3b3",
   "metadata": {},
   "source": [
    "## TUNED GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8d125-7d1b-4d34-8c9c-6874f6871ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1. Сначала определяем функцию загрузки модели\n",
    "def load_model(\n",
    "    model_path: str = '../trained_model/',\n",
    "    tokenizer_path: str = '../trained_tokenizer/',\n",
    "    device: str = None\n",
    ") -> Tuple[GPT2LMHeadModel, GPT2Tokenizer]:\n",
    "    \"\"\"\n",
    "    Загружает модель и токенизатор с обработкой ошибок\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Загрузка модели на устройство: {device}\")\n",
    "\n",
    "        tokenizer3 = GPT2Tokenizer.from_pretrained(\n",
    "            tokenizer_path,\n",
    "            padding_side='left'\n",
    "        )\n",
    "        \n",
    "        if tokenizer3.pad_token is None:\n",
    "            tokenizer3.pad_token = tokenizer3.eos_token\n",
    "\n",
    "        model3 = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "        model3.eval()\n",
    "        \n",
    "        return model3, tokenizer3\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка загрузки модели: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 2. Затем определяем основную функцию генерации\n",
    "def generate_response(\n",
    "    model3: GPT2LMHeadModel,\n",
    "    tokenizer3: GPT2Tokenizer,\n",
    "    context: str,\n",
    "    sep_token: str = \"<|sep|>\",\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 0.7,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.9,\n",
    "    repetition_penalty: float = 1.2,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Основная функция генерации ответа\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not context.strip():\n",
    "            return \"Ошибка: Пустой контекст\"\n",
    "            \n",
    "        prompt = f\"{context.strip()} {sep_token}\"\n",
    "        device = model3.device\n",
    "\n",
    "        inputs = tokenizer3(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=tokenizer3.model_max_length - max_new_tokens\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model3.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer3.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                no_repeat_ngram_size=2,\n",
    "                eos_token_id=tokenizer3.eos_token_id,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        full_text = tokenizer3.decode(output[0], skip_special_tokens=False)\n",
    "        \n",
    "        if sep_token in full_text:\n",
    "            response = full_text.split(sep_token)[1]\n",
    "        else:\n",
    "            response = full_text[len(tokenizer3.decode(inputs.input_ids[0])):]\n",
    "\n",
    "        response = response.split(tokenizer3.eos_token)[0].replace(\"<|endoftext|>\", \"\").strip()\n",
    "        return ' '.join(response.split()) if response else \"Пустой ответ\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка генерации: {str(e)}\")\n",
    "        return \"Извините, произошла внутренняя ошибка\"\n",
    "\n",
    "# 3. Затем определяем обертку с загрузкой модели\n",
    "def generate_tuned_gpt2_response(\n",
    "    context: str,\n",
    "    model_path: str = '../trained_model/',\n",
    "    tokenizer_path: str = '../trained_tokenizer/',\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Полный цикл генерации с автоматической загрузкой модели\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model3, tokenizer3 = load_model(model_path, tokenizer_path)\n",
    "        return generate_response(model3, tokenizer3, context, **kwargs)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка в процессе генерации: {str(e)}\")\n",
    "        return \"Извините, произошла внутренняя ошибка\"\n",
    "\n",
    "\n",
    "\n",
    "q3 = \"Как настроить VPN на Android?\"\n",
    "print(f\"{q3}:\\n{generate_tuned_gpt2_response(q3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d2606-445d-48bb-b332-eef4826339a3",
   "metadata": {},
   "source": [
    "## Vanilla GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06240758-0288-46e4-8a6f-7e05ddf86860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_vanilla_gpt2_response(\n",
    "    context: str,\n",
    "    model_name: str = \"gpt2\",\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.95,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Генерация ответа с использованием оригинальной GPT-2 модели\n",
    "    \n",
    "    Параметры:\n",
    "        context: Входной контекст/вопрос\n",
    "        model_name: Название стандартной модели (gpt2, gpt2-medium и т.д.)\n",
    "        max_new_tokens: Максимальная длина ответа\n",
    "        temperature: Параметр \"творчества\" (0.1-1.5)\n",
    "        top_k: Ограничение словаря для top-k выборки\n",
    "        top_p: Фильтрация ядерной выборки\n",
    "        \n",
    "    Возвращает:\n",
    "        Сгенерированный ответ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Определение устройства\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Загрузка стандартной модели и токенизатора\n",
    "        tokenizer4 = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        model4 = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "        model4.eval()\n",
    "        \n",
    "        # Подготовка ввода\n",
    "        inputs = tokenizer4.encode(\n",
    "            context + \"\\nresponse:\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Генерация ответа\n",
    "        with torch.no_grad():\n",
    "            output = model4.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer4.eos_token_id,\n",
    "                do_sample=True,\n",
    "            )\n",
    "        \n",
    "        # Декодирование и постобработка\n",
    "        full_text = tokenizer4.decode(output[0], skip_special_tokens=True)\n",
    "        response = full_text[len(tokenizer4.decode(inputs[0])):].strip()\n",
    "        \n",
    "        return response.split(\"\\n\")[0]  # Берем первую строку ответа\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка генерации: {str(e)}\")\n",
    "        return \"Не удалось получить ответ\"\n",
    "\n",
    "\n",
    "\n",
    "q4 = \"Как настроить VPN на Android?\"\n",
    "print(f\"{q4}:\\n{generate_vanilla_gpt2_response(q4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f5f32-0d0a-46a7-851b-438f49378a88",
   "metadata": {},
   "source": [
    "## Оценка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b8570-7329-47a9-99df-171c7dc26d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What a lovely day today!\"\n",
    "print(generate_tuned_llama_response(prompt))\n",
    "print(generate_vanilla_llama_response(prompt))\n",
    "print(generate_tuned_gpt2_response(prompt))\n",
    "print(generate_vanilla_lgpt2_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9de1bf-6f66-46ac-9689-7823523fde5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Настройка визуализации\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"husl\")\n",
    "COLORS = {'BLEU': '#3498db', 'ROUGE-L': '#e74c3c', 'BERTScore': '#2ecc71'}\n",
    "\n",
    "# Конфигурация логирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "def load_and_prepare_data(file_path: str, sample_size: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Загрузка и подготовка данных\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logging.info(f\"Успешно загружено {len(df)} записей\")\n",
    "        \n",
    "        # Очистка данных\n",
    "        df = df.dropna(subset=['context', 'response']).sample(sample_size, random_state=42)\n",
    "        return df.reset_index(drop=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Ошибка загрузки данных: {e}\")\n",
    "        raise\n",
    "\n",
    "def initialize_metrics_tools():\n",
    "    \"\"\"\n",
    "    Инициализация инструментов для вычисления метрик\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'rouge': Rouge(),\n",
    "        'bleu_smoother': SmoothingFunction().method1,\n",
    "        'bertscore': BERTScorer(lang='en', model_type='bert-base-uncased')\n",
    "    }\n",
    "\n",
    "def calculate_single_metrics(reference: str, hypothesis: str, tools: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Вычисление метрик для одной пары текст-ответ\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    try:  # BLEU Score\n",
    "        metrics['BLEU'] = sentence_bleu(\n",
    "            [reference.split()],\n",
    "            hypothesis.split(),\n",
    "            smoothing_function=tools['bleu_smoother']\n",
    "        )\n",
    "    except:\n",
    "        metrics['BLEU'] = 0.0\n",
    "\n",
    "    try:  # ROUGE-L Score\n",
    "        if len(reference) > 0 and len(hypothesis) > 0:\n",
    "            metrics['ROUGE-L'] = tools['rouge'].get_scores(hypothesis, reference)[0]['rouge-l']['f']\n",
    "        else:\n",
    "            metrics['ROUGE-L'] = 0.0\n",
    "    except:\n",
    "        metrics['ROUGE-L'] = 0.0\n",
    "\n",
    "    try:  # BERTScore\n",
    "        _, _, metrics['BERTScore'] = tools['bertscore'].score([hypothesis], [reference])\n",
    "        metrics['BERTScore'] = metrics['BERTScore'].mean().item()\n",
    "    except:\n",
    "        metrics['BERTScore'] = 0.0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Конфигурация\n",
    "    DATA_PATH = '../data/processed/context_answer.csv'\n",
    "    MODELS = {\n",
    "        'tuned_llama': lambda x: \"This is a fine-tuned Llama response.\",\n",
    "        'vanilla_llama': lambda x: \"Vanilla Llama model response here.\",\n",
    "        'tuned_gpt2': lambda x: \"GPT-2 fine-tuned answer generated.\",\n",
    "        'vanilla_gpt2': lambda x: \"Basic GPT-2 model response.\"\n",
    "    }\n",
    "    \n",
    "    # 1. Загрузка данных\n",
    "    df = load_and_prepare_data(DATA_PATH)\n",
    "    \n",
    "    # 2. Инициализация инструментов\n",
    "    metrics_tools = initialize_metrics_tools()\n",
    "    \n",
    "    # 3. Вычисление метрик\n",
    "    def process_row(row):\n",
    "        results = {}\n",
    "        context = row['context']\n",
    "        reference = row['response']\n",
    "        \n",
    "        for model_name, model_fn in MODELS.items():\n",
    "            hypothesis = model_fn(context)\n",
    "            results[model_name] = calculate_single_metrics(reference, hypothesis, metrics_tools)\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    df['metrics'] = df.apply(process_row, axis=1)\n",
    "    \n",
    "    # 4. Агрегация результатов\n",
    "    aggregated = {\n",
    "        model: {\n",
    "            metric: np.mean([x[model][metric] for x in df['metrics']])\n",
    "            for metric in ['BLEU', 'ROUGE-L', 'BERTScore']\n",
    "        }\n",
    "        for model in MODELS\n",
    "    }\n",
    "    \n",
    "    # 5. Визуализация\n",
    "    plot_comparison(aggregated)\n",
    "    plot_correlation_heatmap(aggregated)\n",
    "    plot_radar_chart(aggregated)\n",
    "\n",
    "def plot_comparison(aggregated: dict):\n",
    "    \"\"\"\n",
    "    Основной график сравнения моделей с сохранением стиля\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Подготовка данных\n",
    "    df_metrics = pd.DataFrame(aggregated).T.reset_index().rename(columns={'index': 'model'})\n",
    "    sorted_metrics = {metric: df_metrics.sort_values(metric, ascending=False) \n",
    "                    for metric in ['BLEU', 'ROUGE-L', 'BERTScore']}\n",
    "    \n",
    "    # Построение графиков\n",
    "    for metric, color in COLORS.items():\n",
    "        models = sorted_metrics[metric]['model'].tolist()\n",
    "        scores = sorted_metrics[metric][metric].values\n",
    "        \n",
    "        # Основная линия\n",
    "        plt.plot(np.arange(1, len(models)+1), scores,\n",
    "                color=color,\n",
    "                linewidth=2.5,\n",
    "                marker='o',\n",
    "                markersize=8,\n",
    "                label=metric)\n",
    "        \n",
    "        # Аннотации\n",
    "        max_score = max(scores)\n",
    "        plt.annotate(f'{max_score:.2f}',\n",
    "                    xy=(1, max_score),\n",
    "                    xytext=(3, max_score+0.02),\n",
    "                    color=color,\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color=color))\n",
    "    \n",
    "    # Настройка оформления\n",
    "    plt.title('Сравнение моделей по метрикам качества', fontsize=14, pad=20)\n",
    "    plt.xlabel('Ранк модели', fontsize=12)\n",
    "    plt.ylabel('Нормализованная оценка', fontsize=12)\n",
    "    plt.xticks(np.arange(1, len(MODELS)+1))\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True, alpha=0.2, linestyle='--')\n",
    "    \n",
    "    # Дополнительные элементы\n",
    "    add_metric_panel(df_metrics)\n",
    "    plt.legend(loc='upper right', frameon=True, facecolor='#f5f5f5')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def add_metric_panel(df_metrics: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Добавление текстовой панели с лучшими моделями\n",
    "    \"\"\"\n",
    "    ax = plt.gca().inset_axes([0.72, 0.68, 0.25, 0.25])\n",
    "    best_models = {\n",
    "        metric: df_metrics.loc[df_metrics[metric].idxmax(), 'model']\n",
    "        for metric in COLORS\n",
    "    }\n",
    "    \n",
    "    text_lines = [\"Лучшие модели:\"] + [f\"{metric}: {model}\" \n",
    "                                      for metric, model in best_models.items()]\n",
    "    ax.text(0.1, 0.8, \"\\n\".join(text_lines), fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "def plot_correlation_heatmap(aggregated: dict):\n",
    "    \"\"\"\n",
    "    Heatmap корреляции между метриками\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(aggregated).T\n",
    "    corr = df.corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt=\".2f\")\n",
    "    plt.title('Корреляция между метриками оценки', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_radar_chart(aggregated: dict):\n",
    "    \"\"\"\n",
    "    Радар-чарт для сравнения профилей моделей\n",
    "    \"\"\"\n",
    "    categories = list(COLORS.keys())\n",
    "    N = len(categories)\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    \n",
    "    # Настройка осей\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    \n",
    "    # Построение для каждой модели\n",
    "    for model, scores in aggregated.items():\n",
    "        values = [scores[metric] for metric in categories]\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, label=model)\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Настройка оформления\n",
    "    plt.title('Профиль метрик по моделям', y=1.08, fontsize=14)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
